{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Model \n",
    "Here we implement a Sequence to Sequence Model for the problem of language translation and in that we explicitly implement an encoder and decoder program for model training and hence evaluate our model to see its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "Here we import the data and preprocess our data so that we could train our \n",
    "model on that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the encodings from spacy to specific trainable information for the datasets \n",
    "import spacy\n",
    "spacy_german = spacy.load('de_core_news_sm')\n",
    "spacy_english = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the datasets from torchtext to train out model\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "\n",
    "def tokenize_german(text):\n",
    "    return [token.text for token in spacy_german.tokenizer(text)]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    return [token.text for token in spacy_english.tokenizer(text)][::-1]\n",
    "\n",
    "SOURCE = data.Field(tokenize = tokenize_english,init_token = '<sos>',eos_token = '<eos>',lower = True)\n",
    "TARGET = data.Field(tokenize = tokenize_german,init_token = '<sos>',eos_token = '<eos>',lower = True)\n",
    "\n",
    "train_data , valid_data , test_data = Multi30k.splits(exts = ('.en','.de'),fields = (SOURCE,TARGET))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning dataset size: 29000\n",
      "Validation dataset size: 1014\n",
      "Testing dataset size: 1000\n"
     ]
    }
   ],
   "source": [
    "## Checking the size of the respective datasets\n",
    "print(\"Traning dataset size: \" + str(len(train_data.examples)))\n",
    "print(\"Validation dataset size: \" + str(len(valid_data.examples)))\n",
    "print(\"Testing dataset size: \" + str(len(test_data.examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English (Source) Vocabulary Size: 5893\n",
      "German (Target) Vocabulary Size: 7854\n"
     ]
    }
   ],
   "source": [
    "## Hence developing the vocabulary from the training data by defining both the Source and Target\n",
    "SOURCE.build_vocab(train_data,min_freq = 2)\n",
    "TARGET.build_vocab(train_data,min_freq = 2)\n",
    "print(\"English (Source) Vocabulary Size: \" + str(len(SOURCE.vocab)))\n",
    "print(\"German (Target) Vocabulary Size: \" + str(len(TARGET.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "## Using cuda for parrallel processing \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "## defining the iterators for yielding the mini-batches for training the model\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data,valid_data,test_data),batch_size = batch_size,device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Encoder and Decoder\n",
    "Here we implement our respective encoders and decoders to use in the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "## Hence building the Encoder and Decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dims,emb_dims,hid_dims,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dims = hid_dims\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dims,emb_dims)\n",
    "        self.rnn = nn.LSTM(emb_dims,hid_dims,n_layers,dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (h,cell) = self.rnn(embedded)\n",
    "        return h,cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dims,emb_dims,hid_dims,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dims = output_dims\n",
    "        self.hid_dims = hid_dims\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dims,emb_dims)\n",
    "        self.rnn = nn.LSTM(emb_dims,hid_dims,n_layers,dropout = dropout)\n",
    "        self.fc_out = nn.Linear(hid_dims,output_dims)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,inputs,h,cell):\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        \n",
    "        output , (h,cell) = self.rnn(embedded,(h,cell))\n",
    "        \n",
    "        pred = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        return pred,h,cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Sequence to Sequence Model\n",
    "Here we implement our full Sequence to Sequence Model using the respective encoders and decoders and hence training on the preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "## defining and implementing the full model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self,src,trg,teacher_forcing_rate = 0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        target_length = trg.shape[0]\n",
    "        target_vocab_size = self.decoder.output_dims\n",
    "        \n",
    "        outputs = torch.zeros(target_length,batch_size,target_vocab_size).to(self.device)\n",
    "        \n",
    "        h , cell = self.encoder(src)\n",
    "        inputs = trg[0,:]\n",
    "        \n",
    "        for t in range(1,target_length):\n",
    "            output , h ,cell = self.decoder(inputs , h,cell)\n",
    "            outputs[t] = output\n",
    "            top = output.argmax(1)\n",
    "            inputs = trg[t] if (random.random() < teacher_forcing_rate) else top\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "## defining the model's hyperparameters\n",
    "## We could easily tweak them to improve our model's performance\n",
    "input_dimensions = len(SOURCE.vocab)\n",
    "output_dimensions = len(TARGET.vocab)\n",
    "encoder_embedding_dimensions = 256\n",
    "decoder_embedding_dimensions = 256\n",
    "hidden_layer_dimensions = 512\n",
    "number_of_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "\n",
    "encod = Encoder(input_dimensions,encoder_embedding_dimensions,hidden_layer_dimensions,number_of_layers,encoder_dropout)\n",
    "decod = Decoder(output_dimensions,decoder_embedding_dimensions,hidden_layer_dimensions,number_of_layers,decoder_dropout)\n",
    "\n",
    "model = Seq2Seq(encod,decod,device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Here we respectively train our model using the helper functions defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | Time: 77.0s\n",
      "\tTrain Loss:  4.7413\n",
      "\t Val. Loss:  4.6916\n",
      "Epoch:  2 | Time: 80.0s\n",
      "\tTrain Loss:  4.0156\n",
      "\t Val. Loss:  4.3738\n",
      "Epoch:  3 | Time: 83.0s\n",
      "\tTrain Loss:  3.6573\n",
      "\t Val. Loss:  4.0608\n",
      "Epoch:  4 | Time: 83.0s\n",
      "\tTrain Loss:  3.3679\n",
      "\t Val. Loss:  3.9112\n",
      "Epoch:  5 | Time: 83.0s\n",
      "\tTrain Loss:  3.1598\n",
      "\t Val. Loss:  3.7696\n",
      "Epoch:  6 | Time: 87.0s\n",
      "\tTrain Loss:  2.9672\n",
      "\t Val. Loss:  3.6760\n",
      "Epoch:  7 | Time: 85.0s\n",
      "\tTrain Loss:  2.8061\n",
      "\t Val. Loss:  3.6345\n",
      "Epoch:  8 | Time: 84.0s\n",
      "\tTrain Loss:  2.6590\n",
      "\t Val. Loss:  3.5427\n",
      "Epoch:  9 | Time: 84.0s\n",
      "\tTrain Loss:  2.5385\n",
      "\t Val. Loss:  3.5549\n",
      "Epoch:  10 | Time: 84.0s\n",
      "\tTrain Loss:  2.4032\n",
      "\t Val. Loss:  3.5731\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "## initialize weights for training\n",
    "def initialize_weights(m):\n",
    "    for name,param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data,-0.1,0.1)\n",
    "        \n",
    "model.apply(initialize_weights)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TARGET.vocab.stoi[TARGET.pad_token])\n",
    "\n",
    "## define the training function\n",
    "def train(model,iterator,optimizer,criterion,clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i,batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src,trg)\n",
    "        output_dims = output.shape[-1]\n",
    "        output = output[1:].view(-1,output_dims)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output,trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "## defining the evaluation function\n",
    "def evaluate(model,iterator,criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            output = model(src,trg,0)\n",
    "            output_dims = output.shape[-1]\n",
    "            output = output[1:].view(-1,output_dims)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output,trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "epochs = 10\n",
    "grad_clip = 1\n",
    "\n",
    "lowest_validation_loss = float('inf')\n",
    "\n",
    "## training for the required number of epochs\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model,train_iterator,optimizer,criterion,grad_clip)\n",
    "    valid_loss = evaluate(model,valid_iterator,criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    if valid_loss < lowest_validation_loss:\n",
    "        lowest_validation_loss = valid_loss\n",
    "        torch.save(model.state_dict(),'seq2seq.pt')\n",
    "        \n",
    "    ## checking models accuracies respectively to ensure model is being trained correctly\n",
    "    print(f'Epoch: {epoch+1 : 02} | Time: {np.round(end_time-start_time,0)}s')\n",
    "    print(f'\\tTrain Loss: {train_loss: .4f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss: .4f}')\n",
    "    ## Hence our models performs well enough on the training and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "Here we evaluate our model on the test iterator data that we would yield from the corpus of dataset made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.5030\n"
     ]
    }
   ],
   "source": [
    "## Hence we evaluate our model on the testing dataset\n",
    "model.load_state_dict(torch.load('seq2seq.pt'))\n",
    "test_loss = evaluate(model,test_iterator,criterion)\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "## hence our loss on the testing dataset is good enough\n",
    "## our model generalized well enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Input: ['two', 'men', 'wearing', 'hats', '.']\n",
      "Correct German Output: ['zwei', 'männer', 'mit', 'mützen', '.']\n",
      "Predicted German Output: ['zwei', 'männer', 'tragen', 'sonnenbrillen', '.']\n",
      "\n",
      "\n",
      "English Input: ['young', 'woman', 'climbing', 'rock', 'face']\n",
      "Correct German Output: ['junge', 'frau', 'klettert', 'auf', 'felswand']\n",
      "Predicted German Output: ['eine', 'junge', 'frau', 'klettert', 'einen']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## this is just an helper function which translate our models outputs and checks with the true output\n",
    "def translate(model, iterator, limit = 2):    \n",
    "    model.eval()  \n",
    "    epoch_loss = 0    \n",
    "    with torch.no_grad():    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            if i < limit :\n",
    "                \n",
    "                src = batch.src\n",
    "                trg = batch.trg\n",
    "\n",
    "                output = model(src, trg, 0)\n",
    "                preds = torch.tensor([[torch.argmax(x).item()] for x in output])\n",
    "                \n",
    "                print('English Input: ' + str([SOURCE.vocab.itos[x] for x in src][1:-1][::-1]))\n",
    "                print('Correct German Output: ' + str([TARGET.vocab.itos[x] for x in trg][1:-1]))\n",
    "                print('Predicted German Output: ' + str([TARGET.vocab.itos[x] for x in preds][1:-1]))\n",
    "                print('\\n')\n",
    "\n",
    "## getting the yields from the testing dataset\n",
    "_, _, eval_iterator = data.BucketIterator.splits((train_data, valid_data, test_data),batch_size = 1, device = device)\n",
    "\n",
    "## Hence checking the models output \n",
    "output = translate(model, eval_iterator)\n",
    "## hence we can see that our model generalizes well enough the provided dataset and could be used in transfer learning \n",
    "## new problems also"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
