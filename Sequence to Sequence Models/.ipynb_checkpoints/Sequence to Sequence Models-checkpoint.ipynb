{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Model \n",
    "Here we implement a Sequence to Sequence Model for the problem of language translation and in that we explicitly implement an encoder and decoder program for model training and hence evaluate our model to see its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "Here we import the data and preprocess our data so that we could train our \n",
    "model on that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the encodings from spacy to specific trainable information for the datasets \n",
    "import spacy\n",
    "spacy_german = spacy.load('de_core_news_sm')\n",
    "spacy_english = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torchtext\\data\\field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "C:\\Users\\Asus\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torchtext\\data\\example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "## Importing the datasets from torchtext to train out model\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "\n",
    "def tokenize_german(text):\n",
    "    return [token.text for token in spacy_german.tokenizer(text)]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    return [token.text for token in spacy_english.tokenizer(text)][::-1]\n",
    "\n",
    "SOURCE = data.Field(tokenize = tokenize_english,init_token = '<sos>',eos_token = '<eos>',lower = True)\n",
    "TARGET = data.Field(tokenize = tokenize_german,init_token = '<sos>',eos_token = '<eos>',lower = True)\n",
    "\n",
    "train_data , valid_data , test_data = Multi30k.splits(exts = ('.en','.de'),fields = (SOURCE,TARGET))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning dataset size: 29000\n",
      "Validation dataset size: 1014\n",
      "Testing dataset size: 1000\n"
     ]
    }
   ],
   "source": [
    "## Checking the size of the respective datasets\n",
    "print(\"Traning dataset size: \" + str(len(train_data.examples)))\n",
    "print(\"Validation dataset size: \" + str(len(valid_data.examples)))\n",
    "print(\"Testing dataset size: \" + str(len(test_data.examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English (Source) Vocabulary Size: 5893\n",
      "German (Target) Vocabulary Size: 7854\n"
     ]
    }
   ],
   "source": [
    "## Hence developing the vocabulary from the training data by defining both the Source and Target\n",
    "SOURCE.build_vocab(train_data,min_freq = 2)\n",
    "TARGET.build_vocab(train_data,min_freq = 2)\n",
    "print(\"English (Source) Vocabulary Size: \" + str(len(SOURCE.vocab)))\n",
    "print(\"German (Target) Vocabulary Size: \" + str(len(TARGET.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torchtext\\data\\iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "## Using cuda for parrallel processing \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "## defining the iterators for yielding the mini-batches for training the model\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data,valid_data,test_data),batch_size = batch_size,device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Encoder and Decoder\n",
    "Here we implement our respective encoders and decoders to use in the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "## Hence building the Encoder and Decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dims,emb_dims,hid_dims,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dims = hid_dims\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dims,emb_dims)\n",
    "        self.rnn = nn.LSTM(emb_dims,hid_dims,n_layers,dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (h,cell) = self.rnn(embedded)\n",
    "        return h,cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dims,emb_dims,hid_dims,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dims = output_dims\n",
    "        self.hid_dims = hid_dims\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dims,emb_dims)\n",
    "        self.rnn = nn.LSTM(emb_dims,hid_dims,n_layers,dropout = dropout)\n",
    "        self.fc_out = nn.Linear(hid_dims,output_dims)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,inputs,h,cell):\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        \n",
    "        output , (h,cell) = self.rnn(embedded,(h,cell))\n",
    "        \n",
    "        pred = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        return pred,h,cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Sequence to Sequence Model\n",
    "Here we implement our full Sequence to Sequence Model using the respective encoders and decoders and hence training on the preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "## defining and implementing the full model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self,src,trg,teacher_forcing_rate = 0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        target_length = trg.shape[0]\n",
    "        target_vocab_size = self.decoder.output_dims\n",
    "        \n",
    "        outputs = torch.zeros(target_length,batch_size,target_vocab_size).to(self.device)\n",
    "        \n",
    "        h , cell = self.encoder(src)\n",
    "        inputs = trg[0,:]\n",
    "        \n",
    "        for t in range(1,target_length):\n",
    "            output , h ,cell = self.decoder(inputs , h,cell)\n",
    "            outputs[t] = output\n",
    "            top = output.argmax(1)\n",
    "            inputs = trg[t] if (random.random() < teacher_forcing_rate) else top\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "## defining the model's hyperparameters\n",
    "## We could easily tweak them to improve our model's performance\n",
    "input_dimensions = len(SOURCE.vocab)\n",
    "output_dimensions = len(TARGET.vocab)\n",
    "encoder_embedding_dimensions = 256\n",
    "decoder_embedding_dimensions = 256\n",
    "hidden_layer_dimensions = 512\n",
    "number_of_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "\n",
    "encod = Encoder(input_dimensions,encoder_embedding_dimensions,hidden_layer_dimensions,number_of_layers,encoder_dropout)\n",
    "decod = Decoder(output_dimensions,decoder_embedding_dimensions,hidden_layer_dimensions,number_of_layers,decoder_dropout)\n",
    "\n",
    "model = Seq2Seq(encod,decod,device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Here we respectively train our model using the helper functions defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torchtext\\data\\batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | Time: 81.0s\n",
      "\tTrain Loss:  4.7260\n",
      "\t Val. Loss:  4.6445\n",
      "Epoch:  2 | Time: 78.0s\n",
      "\tTrain Loss:  4.0280\n",
      "\t Val. Loss:  4.3099\n",
      "Epoch:  3 | Time: 82.0s\n",
      "\tTrain Loss:  3.6661\n",
      "\t Val. Loss:  4.0707\n",
      "Epoch:  4 | Time: 84.0s\n",
      "\tTrain Loss:  3.3996\n",
      "\t Val. Loss:  3.9362\n",
      "Epoch:  5 | Time: 92.0s\n",
      "\tTrain Loss:  3.1821\n",
      "\t Val. Loss:  3.7796\n",
      "Epoch:  6 | Time: 93.0s\n",
      "\tTrain Loss:  2.9969\n",
      "\t Val. Loss:  3.7215\n",
      "Epoch:  7 | Time: 91.0s\n",
      "\tTrain Loss:  2.8408\n",
      "\t Val. Loss:  3.6440\n",
      "Epoch:  8 | Time: 94.0s\n",
      "\tTrain Loss:  2.6922\n",
      "\t Val. Loss:  3.6399\n",
      "Epoch:  9 | Time: 96.0s\n",
      "\tTrain Loss:  2.5497\n",
      "\t Val. Loss:  3.6287\n",
      "Epoch:  10 | Time: 93.0s\n",
      "\tTrain Loss:  2.4388\n",
      "\t Val. Loss:  3.5828\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "## initialize weights for training\n",
    "def initialize_weights(m):\n",
    "    for name,param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data,-0.1,0.1)\n",
    "        \n",
    "model.apply(initialize_weights)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TARGET.vocab.stoi[TARGET.pad_token])\n",
    "\n",
    "## define the training function\n",
    "def train(model,iterator,optimizer,criterion,clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i,batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src,trg)\n",
    "        output_dims = output.shape[-1]\n",
    "        output = output[1:].view(-1,output_dims)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output,trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "## defining the evaluation function\n",
    "def evaluate(model,iterator,criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            output = model(src,trg,0)\n",
    "            output_dims = output.shape[-1]\n",
    "            output = output[1:].view(-1,output_dims)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output,trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "epochs = 10\n",
    "grad_clip = 1\n",
    "\n",
    "lowest_validation_loss = float('inf')\n",
    "\n",
    "## training for the required number of epochs\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model,train_iterator,optimizer,criterion,grad_clip)\n",
    "    valid_loss = evaluate(model,valid_iterator,criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    if valid_loss < lowest_validation_loss:\n",
    "        lowest_validation_loss = valid_loss\n",
    "        torch.save(model.state_dict(),'seq2seq.pt')\n",
    "        \n",
    "    ## checking models accuracies respectively to ensure model is being trained correctly\n",
    "    print(f'Epoch: {epoch+1 : 02} | Time: {np.round(end_time-start_time,0)}s')\n",
    "    print(f'\\tTrain Loss: {train_loss: .4f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss: .4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "Here we evaluate our model on the test iterator data that we would yield from the corpus of dataset made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torchtext\\data\\batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.5505\n"
     ]
    }
   ],
   "source": [
    "## Hence we evaluate our model\n",
    "model.load_state_dict(torch.load('seq2seq.pt'))\n",
    "test_loss = evaluate(model,test_iterator,criterion)\n",
    "print(f'Test loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torchtext\\data\\iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Input: ['two', 'men', 'wearing', 'hats', '.']\n",
      "Correct German Output: ['zwei', 'männer', 'mit', 'mützen', '.']\n",
      "Predicted German Output: ['zwei', 'männer', 'mit', 'sonnenbrillen', '.']\n",
      "\n",
      "\n",
      "English Input: ['young', 'woman', 'climbing', 'rock', 'face']\n",
      "Correct German Output: ['junge', 'frau', 'klettert', 'auf', 'felswand']\n",
      "Predicted German Output: ['eine', 'junge', 'frau', 'klettert', 'einen']\n",
      "\n",
      "\n",
      "English Input: ['a', 'woman', 'is', 'playing', 'volleyball', '.']\n",
      "Correct German Output: ['eine', 'frau', 'spielt', 'volleyball', '.']\n",
      "Predicted German Output: ['eine', 'frau', 'spielt', 'tennis', '.']\n",
      "\n",
      "\n",
      "English Input: ['three', 'men', 'are', 'walking', 'up', 'hill', '.']\n",
      "Correct German Output: ['drei', 'männer', 'gehen', 'bergauf', '.']\n",
      "Predicted German Output: ['drei', 'männer', 'gehen', 'einen', 'berg']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate(model, iterator, limit = 4):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            if i < limit :\n",
    "                \n",
    "                src = batch.src\n",
    "                trg = batch.trg\n",
    "\n",
    "                output = model(src, trg, 0)\n",
    "                preds = torch.tensor([[torch.argmax(x).item()] for x in output])\n",
    "                \n",
    "                print('English Input: ' + str([SOURCE.vocab.itos[x] for x in src][1:-1][::-1]))\n",
    "                print('Correct German Output: ' + str([TARGET.vocab.itos[x] for x in trg][1:-1]))\n",
    "                print('Predicted German Output: ' + str([TARGET.vocab.itos[x] for x in preds][1:-1]))\n",
    "                print('\\n')\n",
    "                \n",
    "_, _, eval_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = 1, \n",
    "    device = device)\n",
    "\n",
    "output = translate(model, eval_iterator)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
